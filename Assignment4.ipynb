{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSSS\n",
      "SHSH\n",
      "SSSS\n",
      "HSSF\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "frozenLake4x4 = 'SSSS\\nSHSH\\nSSSS\\nHSSF'\n",
    "\n",
    "\n",
    "print(frozenLake4x4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "# DISCOUNT = 1.0\n",
    "# STEP_REWARD = 0.0\n",
    "# LOSE_REWARD = 0.0\n",
    "# WIN_REWARD = 1.0\n",
    "\n",
    "DISCOUNT = .8\n",
    "STEP_REWARD = 0\n",
    "LOSE_REWARD = 0\n",
    "WIN_REWARD = 1.0\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] not in '#H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col-1,0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a == UP:\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                rew = float(newletter == b'G')\n",
    "                                li.append((1.0/3.0, newstate, rew, done))\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G')\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n",
    "            \n",
    "def value_iteration(env, max_iterations=100000, lmbda=0.9):\n",
    "  stateValue = [0 for i in range(env.nS)]\n",
    "  newStateValue = stateValue.copy()\n",
    "  for i in range(max_iterations):\n",
    "    for state in range(env.nS):\n",
    "      action_values = []      \n",
    "      for action in range(env.nA):\n",
    "        state_value = 0\n",
    "        for i in range(len(env.P[state][action])):\n",
    "          prob, next_state, reward, done = env.P[state][action][i]\n",
    "          state_action_value = prob * (reward + lmbda*stateValue[next_state])\n",
    "          state_value += state_action_value\n",
    "        action_values.append(state_value)      #the value of each action\n",
    "        best_action = np.argmax(np.asarray(action_values))   # choose the action which gives the maximum value\n",
    "        newStateValue[state] = action_values[best_action]  #update the value of the state\n",
    "    if i > 1000: \n",
    "      if sum(stateValue) - sum(newStateValue) < 1e-04:   # if there is negligible difference break the loop\n",
    "        break\n",
    "#         print(i)\n",
    "    else:\n",
    "      stateValue = newStateValue.copy()\n",
    "  return stateValue\n",
    "   \n",
    "def get_policy(env,stateValue, lmbda=0.9):\n",
    "  policy = [0 for i in range(env.nS)]\n",
    "  for state in range(env.nS):\n",
    "    action_values = []\n",
    "    for action in range(env.nA):\n",
    "      action_value = 0\n",
    "      for i in range(len(env.P[state][action])):\n",
    "        prob, next_state, r, _ = env.P[state][action][i]\n",
    "        action_value += prob * (r + lmbda * stateValue[next_state])\n",
    "      action_values.append(action_value)\n",
    "    best_action = np.argmax(np.asarray(action_values))\n",
    "    policy[state] = best_action\n",
    "  return policy \n",
    "\n",
    "def get_score(env, policy, episodes=1000):\n",
    "  misses = 0\n",
    "  steps_list = []\n",
    "  for episode in range(episodes):\n",
    "    observation = env.reset()\n",
    "    steps=0\n",
    "    while True:\n",
    "      \n",
    "      action = policy[observation]\n",
    "      observation, reward, done, _ = env.step(action)\n",
    "      steps+=1\n",
    "      if done and reward == 1:\n",
    "        # print('You have got the fucking Frisbee after {} steps'.format(steps))\n",
    "        steps_list.append(steps)\n",
    "        break\n",
    "      elif done and reward == 0:\n",
    "        # print(\"You fell in a hole!\")\n",
    "        misses += 1\n",
    "        break\n",
    "  print('----------------------------------------------')\n",
    "  print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
    "  print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))\n",
    "  print('----------------------------------------------')\n",
    "\n",
    "def random_policy(env):\n",
    "    return np.random.randint(0, 4, size=env.nS)\n",
    "\n",
    "def one_step_lookahead(env, s, value_function):\n",
    "    action_values = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        value = avg_reward(env, s, a)\n",
    "        for p, next_s, _, _ in env.P[s][a]:\n",
    "            value += DISCOUNT * p * value_function[next_s]\n",
    "        action_values[a] = value\n",
    "    return action_values\n",
    "    \n",
    "def evaluate_policy(env, policy, max_backups=1000, tol=1e-6):\n",
    "    old_value = np.zeros(env.nS)\n",
    "    for i in range(max_backups):\n",
    "        new_value = np.zeros(env.nS)\n",
    "        for s in range(env.nS):\n",
    "            action_values = one_step_lookahead(env, s, old_value)\n",
    "            new_value[s] = action_values[policy[s]]\n",
    "        if np.max(np.abs(new_value-old_value)) < tol:\n",
    "            break\n",
    "        old_value = new_value\n",
    "    return new_value\n",
    "\n",
    "def greedy_policy(env, value_function):\n",
    "    policy = np.zeros(env.nS, dtype=np.int32)\n",
    "    for s in range(env.nS):\n",
    "        action_values = one_step_lookahead(env, s, value_function)\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, max_steps=100):\n",
    "    old_policy = random_policy(env)\n",
    "    for i in range(max_steps):\n",
    "        value_function = evaluate_policy(env, old_policy)\n",
    "        new_policy = greedy_policy(env, value_function)\n",
    "        \n",
    "        if np.array_equal(new_policy, old_policy):\n",
    "            break\n",
    "        old_policy = new_policy\n",
    "    return old_policy, value_function\n",
    "\n",
    "def avg_reward(env, s, a):\n",
    "    avg_reward = 0\n",
    "    for prob, next_s, reward, done in env.P[s][a]:\n",
    "        if not done:\n",
    "            avg_reward += prob * STEP_REWARD\n",
    "        elif reward == 0.0:\n",
    "#             avg_reward += prob * (-5)\n",
    "            avg_reward += prob * LOSE_REWARD\n",
    "        else:\n",
    "#             avg_reward += prob * 10\n",
    "            avg_reward += prob * WIN_REWARD\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'R'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-311eba26579e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FrozenLake-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# env = FrozenLakeEnv(map_name='8x8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(env.P[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# values = value_iteration(frozen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'R'"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "print(env.R)\n",
    "# env = FrozenLakeEnv(map_name='8x8')\n",
    "# print(env.P[0])\n",
    "# values = value_iteration(frozen)\n",
    "# p1 = get_policy(frozen, values)\n",
    "# p2 = policy_iteration(frozen, max_steps=50)\n",
    "# print(np.array(p1))\n",
    "# print(p2[0])\n",
    "\n",
    "\n",
    "# get_score(frozen, p2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
